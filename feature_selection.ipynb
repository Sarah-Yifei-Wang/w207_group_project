{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandasql as ps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Shape : (137166, 23)\n",
      "Index(['Unnamed: 0', 'Zip Code', 'Month of Period End', 'Median Sale Price',\n",
      "       'Median Sale Price MoM ', 'Median Sale Price YoY ', 'Homes Sold',\n",
      "       'Homes Sold MoM ', 'Homes Sold YoY ', 'New Listings',\n",
      "       'New Listings MoM ', 'New Listings YoY ', 'Inventory', 'Inventory MoM ',\n",
      "       ' Inventory YoY ', 'Days on Market', 'Days on Market MoM',\n",
      "       'Days on Market YoY', 'Average Sale To List',\n",
      "       'Average Sale To List MoM ', 'Average Sale To List YoY ',\n",
      "       'record_month', 'PSSF'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#load dataframe from csv\n",
    "merged_sales = pd.read_csv(\"data/zip/merged_sales_stats.csv\", delimiter='\t')\n",
    "\n",
    "#print dataframe shape\n",
    "shape = merged_sales.shape\n",
    "print('\\nDataFrame Shape :', shape)\n",
    "\n",
    "print(merged_sales.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The merged_sales_statistics file has all the statistics as columns and zip codes and dates as the rows. I tried to transpose these but it got weird. So let's get some stuff from the merged_sales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1688, 682)\n",
      "Index(['Unnamed: 0', 'Zip Code', 'June 2009 1bd', 'July 2009 1bd',\n",
      "       'August 2009 1bd', 'September 2009 1bd', 'October 2009 1bd',\n",
      "       'November 2009 1bd', 'December 2009 1bd', 'January 2010 1bd',\n",
      "       ...\n",
      "       'December 2019 5bd', 'January 2020 5bd', 'February 2020 5bd',\n",
      "       'March 2020 5bd', 'April 2020 5bd', 'May 2020 5bd', 'June 2020 5bd',\n",
      "       'July 2020 5bd', 'August 2020 5bd', 'September 2020 5bd'],\n",
      "      dtype='object', length=682)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#load dataframe from csv\n",
    "merged_bedroom_data = pd.read_csv(\"data/zip/merged_bedrooms.csv\", delimiter='\t')\n",
    "print(merged_bedroom_data.shape)\n",
    "print(merged_bedroom_data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41, 207)\n",
      "0     29.100000\n",
      "1     31.900000\n",
      "2     30.700001\n",
      "3     37.599998\n",
      "4     33.000000\n",
      "5     36.099998\n",
      "6     30.799999\n",
      "7     33.700001\n",
      "8     29.500000\n",
      "9     29.100000\n",
      "10    33.299999\n",
      "11    30.299999\n",
      "12    32.099998\n",
      "13    31.799999\n",
      "14    27.299999\n",
      "15    32.799999\n",
      "16    29.900000\n",
      "17    42.599998\n",
      "18    32.099998\n",
      "19    30.799999\n",
      "20    33.200001\n",
      "21    32.400002\n",
      "22    33.000000\n",
      "23    32.700001\n",
      "24    22.299999\n",
      "25    31.500000\n",
      "26    33.099998\n",
      "27    27.600000\n",
      "28    33.000000\n",
      "29    27.600000\n",
      "30    32.200001\n",
      "31    27.200001\n",
      "32    30.600000\n",
      "33    31.500000\n",
      "34    31.700001\n",
      "35    33.500000\n",
      "36    26.700001\n",
      "37    29.400000\n",
      "38    33.200001\n",
      "39    31.000000\n",
      "40    30.600000\n",
      "Name: SE_A18003_001, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "now for our demographic info, it's from 2019 so not super up to date, but pretty close and there's a lot of it\n",
    "'''\n",
    "demographics = pd.read_csv(\"data/zip/ca_demographics.csv\", delimiter='\t')\n",
    "zip_to_county = pd.read_csv(\"data/zip/zip_to_county.csv\", delimiter='\t')\n",
    "\n",
    "print(demographics.shape) # a lot of blank columns\n",
    "\n",
    "\n",
    "print(demographics[\"SE_A18003_001\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the column names are gibberish, so we'll need to use the [ca_demographics_key.txt](data/zip/ca_demographics_key.txt) to make sense of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1763, 217)\n"
     ]
    }
   ],
   "source": [
    "zipdemo = pd.read_csv(\"data/zip/ca_zip_demograhics.csv\", delimiter='\t')\n",
    "print(zipdemo.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot to look at in that dataset, but certainly all the key indicators about the zip should give us at least some picture of what the housing market there might look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Condensing Sale Data to Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove month from \"Month of Period End\"\n",
    "only_year_query = \"\"\"SELECT \"Zip Code\" as zip_code, \n",
    "                    SUBSTR(\"Month of Period End\" ,-4) as year,\n",
    "                    \"Median Sale Price\" as med_sale_price,\n",
    "                    \"Homes Sold\" as home_sold,\n",
    "                    \"New Listings\" as new_listings,\n",
    "                    \"Inventory\" as inventory,\n",
    "                    \"Days on Market\" as days_on_market,\n",
    "                    \"Average Sale To List\" as avg_sale_to_list,\n",
    "                    PSSF as ppsf\n",
    "                    FROM merged_sales\"\"\"\n",
    "only_year = ps.sqldf(only_year_query, locals())\n",
    "\n",
    "#group by year\n",
    "cond_year_query = \"\"\"SELECT zip_code, \n",
    "                            year,\n",
    "                            AVG(med_sale_price) as med_sale_price,\n",
    "                            SUM(home_sold) as homes_sold,\n",
    "                            SUM(new_listings) as new_listings,\n",
    "                            AVG(inventory) as inventory,\n",
    "                            AVG(days_on_market) as days_on_market,\n",
    "                            AVG(avg_sale_to_list) as avg_sale_to_list,\n",
    "                            AVG(ppsf) as ppsf\n",
    "                    FROM only_year\n",
    "                    GROUP BY year, zip_code\"\"\"\n",
    "cond_year = ps.sqldf(cond_year_query, locals())\n",
    "\n",
    "#filtering to 2018\n",
    "year_18_query = \"\"\"SELECT *\n",
    "                    FROM cond_year\n",
    "                    WHERE year = '2018' \"\"\"\n",
    "year_18 = ps.sqldf(year_18_query, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2018 Sales and Demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip_code</th>\n",
       "      <th>year</th>\n",
       "      <th>med_sale_price</th>\n",
       "      <th>homes_sold</th>\n",
       "      <th>new_listings</th>\n",
       "      <th>inventory</th>\n",
       "      <th>days_on_market</th>\n",
       "      <th>avg_sale_to_list</th>\n",
       "      <th>ppsf</th>\n",
       "      <th>Geo_FIPS</th>\n",
       "      <th>...</th>\n",
       "      <th>SE_A08002B_005</th>\n",
       "      <th>SE_A08002B_006</th>\n",
       "      <th>SE_A10066_001</th>\n",
       "      <th>SE_A10066_002</th>\n",
       "      <th>SE_A10066_003</th>\n",
       "      <th>SE_A10066_004</th>\n",
       "      <th>SE_A10066_005</th>\n",
       "      <th>SE_A10066_006</th>\n",
       "      <th>SE_A10066_007</th>\n",
       "      <th>SE_A10066_008</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90001</td>\n",
       "      <td>2018</td>\n",
       "      <td>4.228333e+05</td>\n",
       "      <td>511</td>\n",
       "      <td>678.0</td>\n",
       "      <td>44.333333</td>\n",
       "      <td>43.166667</td>\n",
       "      <td>100.125000</td>\n",
       "      <td>322.500000</td>\n",
       "      <td>90090001</td>\n",
       "      <td>...</td>\n",
       "      <td>144</td>\n",
       "      <td>118</td>\n",
       "      <td>13815</td>\n",
       "      <td>1834</td>\n",
       "      <td>2083</td>\n",
       "      <td>2594</td>\n",
       "      <td>2513</td>\n",
       "      <td>2045</td>\n",
       "      <td>1330</td>\n",
       "      <td>1416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90002</td>\n",
       "      <td>2018</td>\n",
       "      <td>3.825000e+05</td>\n",
       "      <td>964</td>\n",
       "      <td>1178.0</td>\n",
       "      <td>74.666667</td>\n",
       "      <td>39.916667</td>\n",
       "      <td>99.916667</td>\n",
       "      <td>323.750000</td>\n",
       "      <td>90090002</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>124</td>\n",
       "      <td>12706</td>\n",
       "      <td>2096</td>\n",
       "      <td>2232</td>\n",
       "      <td>2020</td>\n",
       "      <td>2266</td>\n",
       "      <td>1719</td>\n",
       "      <td>895</td>\n",
       "      <td>1478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90003</td>\n",
       "      <td>2018</td>\n",
       "      <td>4.076667e+05</td>\n",
       "      <td>962</td>\n",
       "      <td>1222.0</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>45.416667</td>\n",
       "      <td>99.933333</td>\n",
       "      <td>314.166667</td>\n",
       "      <td>90090003</td>\n",
       "      <td>...</td>\n",
       "      <td>193</td>\n",
       "      <td>129</td>\n",
       "      <td>17127</td>\n",
       "      <td>2594</td>\n",
       "      <td>2479</td>\n",
       "      <td>2966</td>\n",
       "      <td>3345</td>\n",
       "      <td>2757</td>\n",
       "      <td>1476</td>\n",
       "      <td>1510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90004</td>\n",
       "      <td>2018</td>\n",
       "      <td>1.286500e+06</td>\n",
       "      <td>747</td>\n",
       "      <td>1117.0</td>\n",
       "      <td>85.833333</td>\n",
       "      <td>33.833333</td>\n",
       "      <td>99.233333</td>\n",
       "      <td>630.916667</td>\n",
       "      <td>90090004</td>\n",
       "      <td>...</td>\n",
       "      <td>483</td>\n",
       "      <td>463</td>\n",
       "      <td>21971</td>\n",
       "      <td>6379</td>\n",
       "      <td>6385</td>\n",
       "      <td>3842</td>\n",
       "      <td>2854</td>\n",
       "      <td>1539</td>\n",
       "      <td>721</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90005</td>\n",
       "      <td>2018</td>\n",
       "      <td>7.710000e+05</td>\n",
       "      <td>314</td>\n",
       "      <td>446.0</td>\n",
       "      <td>30.666667</td>\n",
       "      <td>30.250000</td>\n",
       "      <td>99.225000</td>\n",
       "      <td>523.000000</td>\n",
       "      <td>90090005</td>\n",
       "      <td>...</td>\n",
       "      <td>431</td>\n",
       "      <td>380</td>\n",
       "      <td>16442</td>\n",
       "      <td>6298</td>\n",
       "      <td>4939</td>\n",
       "      <td>2183</td>\n",
       "      <td>1794</td>\n",
       "      <td>832</td>\n",
       "      <td>293</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>96146</td>\n",
       "      <td>2018</td>\n",
       "      <td>5.807500e+05</td>\n",
       "      <td>249</td>\n",
       "      <td>368.0</td>\n",
       "      <td>62.666667</td>\n",
       "      <td>163.333333</td>\n",
       "      <td>95.691667</td>\n",
       "      <td>486.333333</td>\n",
       "      <td>96196146</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>454</td>\n",
       "      <td>152</td>\n",
       "      <td>215</td>\n",
       "      <td>31</td>\n",
       "      <td>24</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>96148</td>\n",
       "      <td>2018</td>\n",
       "      <td>6.440833e+05</td>\n",
       "      <td>155</td>\n",
       "      <td>156.0</td>\n",
       "      <td>14.583333</td>\n",
       "      <td>90.500000</td>\n",
       "      <td>96.950000</td>\n",
       "      <td>413.666667</td>\n",
       "      <td>96196148</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>210</td>\n",
       "      <td>55</td>\n",
       "      <td>62</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>96150</td>\n",
       "      <td>2018</td>\n",
       "      <td>4.536667e+05</td>\n",
       "      <td>2333</td>\n",
       "      <td>3039.0</td>\n",
       "      <td>246.333333</td>\n",
       "      <td>43.750000</td>\n",
       "      <td>97.008333</td>\n",
       "      <td>307.666667</td>\n",
       "      <td>96196150</td>\n",
       "      <td>...</td>\n",
       "      <td>461</td>\n",
       "      <td>24</td>\n",
       "      <td>11536</td>\n",
       "      <td>3645</td>\n",
       "      <td>4315</td>\n",
       "      <td>1666</td>\n",
       "      <td>1221</td>\n",
       "      <td>427</td>\n",
       "      <td>202</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>96155</td>\n",
       "      <td>2018</td>\n",
       "      <td>2.340000e+05</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>91.333333</td>\n",
       "      <td>145.666667</td>\n",
       "      <td>96196155</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>96161</td>\n",
       "      <td>2018</td>\n",
       "      <td>6.946667e+05</td>\n",
       "      <td>2534</td>\n",
       "      <td>3015.0</td>\n",
       "      <td>289.500000</td>\n",
       "      <td>65.250000</td>\n",
       "      <td>97.816667</td>\n",
       "      <td>367.000000</td>\n",
       "      <td>96196161</td>\n",
       "      <td>...</td>\n",
       "      <td>233</td>\n",
       "      <td>59</td>\n",
       "      <td>6929</td>\n",
       "      <td>1385</td>\n",
       "      <td>2772</td>\n",
       "      <td>1087</td>\n",
       "      <td>1437</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1337 rows × 226 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      zip_code  year  med_sale_price  homes_sold  new_listings   inventory  \\\n",
       "0        90001  2018    4.228333e+05         511         678.0   44.333333   \n",
       "1        90002  2018    3.825000e+05         964        1178.0   74.666667   \n",
       "2        90003  2018    4.076667e+05         962        1222.0   96.000000   \n",
       "3        90004  2018    1.286500e+06         747        1117.0   85.833333   \n",
       "4        90005  2018    7.710000e+05         314         446.0   30.666667   \n",
       "...        ...   ...             ...         ...           ...         ...   \n",
       "1332     96146  2018    5.807500e+05         249         368.0   62.666667   \n",
       "1333     96148  2018    6.440833e+05         155         156.0   14.583333   \n",
       "1334     96150  2018    4.536667e+05        2333        3039.0  246.333333   \n",
       "1335     96155  2018    2.340000e+05           8           NaN         NaN   \n",
       "1336     96161  2018    6.946667e+05        2534        3015.0  289.500000   \n",
       "\n",
       "      days_on_market  avg_sale_to_list        ppsf  Geo_FIPS  ...  \\\n",
       "0          43.166667        100.125000  322.500000  90090001  ...   \n",
       "1          39.916667         99.916667  323.750000  90090002  ...   \n",
       "2          45.416667         99.933333  314.166667  90090003  ...   \n",
       "3          33.833333         99.233333  630.916667  90090004  ...   \n",
       "4          30.250000         99.225000  523.000000  90090005  ...   \n",
       "...              ...               ...         ...       ...  ...   \n",
       "1332      163.333333         95.691667  486.333333  96196146  ...   \n",
       "1333       90.500000         96.950000  413.666667  96196148  ...   \n",
       "1334       43.750000         97.008333  307.666667  96196150  ...   \n",
       "1335      104.000000         91.333333  145.666667  96196155  ...   \n",
       "1336       65.250000         97.816667  367.000000  96196161  ...   \n",
       "\n",
       "     SE_A08002B_005 SE_A08002B_006 SE_A10066_001 SE_A10066_002  SE_A10066_003  \\\n",
       "0               144            118         13815          1834           2083   \n",
       "1                45            124         12706          2096           2232   \n",
       "2               193            129         17127          2594           2479   \n",
       "3               483            463         21971          6379           6385   \n",
       "4               431            380         16442          6298           4939   \n",
       "...             ...            ...           ...           ...            ...   \n",
       "1332              0              9           454           152            215   \n",
       "1333             24              0           210            55             62   \n",
       "1334            461             24         11536          3645           4315   \n",
       "1335              0              0             0             0              0   \n",
       "1336            233             59          6929          1385           2772   \n",
       "\n",
       "      SE_A10066_004 SE_A10066_005  SE_A10066_006 SE_A10066_007 SE_A10066_008  \n",
       "0              2594          2513           2045          1330          1416  \n",
       "1              2020          2266           1719           895          1478  \n",
       "2              2966          3345           2757          1476          1510  \n",
       "3              3842          2854           1539           721           251  \n",
       "4              2183          1794            832           293           103  \n",
       "...             ...           ...            ...           ...           ...  \n",
       "1332             31            24             32             0             0  \n",
       "1333              6            50             26             0            11  \n",
       "1334           1666          1221            427           202            60  \n",
       "1335              0             0              0             0             0  \n",
       "1336           1087          1437            187             0            61  \n",
       "\n",
       "[1337 rows x 226 columns]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#zip code = Geo_ZCTA5\n",
    "\n",
    "sales_demo_query = \"\"\"SELECT year_18.*, zipdemo.*\n",
    "                    FROM year_18\n",
    "                    INNER JOIN zipdemo on zip_code = Geo_ZCTA5\n",
    "                    \"\"\"\n",
    "sales_demo = ps.sqldf(sales_demo_query, locals())\n",
    "sales_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#create avg_sale_to_list groups (split into 5)\n",
    "ntile_DoM_5_query = \"\"\"SELECT *, NTILE(5) OVER (\n",
    "                            ORDER BY days_on_market ASC) as DoM_group      \n",
    "                        FROM sales_demo\"\"\"\n",
    "ntile_DoM_5 = ps.sqldf(ntile_DoM_5_query, locals())\n",
    "\n",
    "\n",
    "#see group maximums\n",
    "minmax_DoM_5_query = \"\"\"SELECT DoM_group, COUNT(zip_code), MAX(days_on_market)\n",
    "                        FROM ntile_DoM\n",
    "                        GROUP BY DoM_group\"\"\"\n",
    "minmax_DoM_5 = ps.sqldf(minmax_DoM_5_query, locals())\n",
    "minmax_DoM_5\n",
    "\n",
    "\n",
    "\n",
    "#Delete non-numeric and unused columns.\n",
    "del_col = ['Geo_FIPS', 'Geo_GEOID', 'Geo_NAME', 'Geo_QName', 'Geo_STUSAB', 'Geo_SUMLEV', \n",
    "           'Geo_GEOCOMP', 'Geo_FILEID', 'Geo_LOGRECNO', 'Geo_US', 'Geo_REGION', 'Geo_DIVISION', \n",
    "          'Geo_STATECE', 'Geo_STATE', 'Geo_COUNTY', 'Geo_COUSUB', 'Geo_PLACE', 'Geo_PLACESE', \n",
    "          'Geo_TRACT', 'Geo_BLKGRP', 'Geo_CONCIT', 'Geo_AIANHH', 'Geo_AIANHHFP', 'Geo_AIHHTLI', \n",
    "          'Geo_AITSCE', 'Geo_AITS', 'Geo_ANRC', 'Geo_CBSA', 'Geo_CSA', 'Geo_METDIV', 'Geo_MACC', \n",
    "          'Geo_MEMI', 'Geo_NECTA', 'Geo_CNECTA', 'Geo_NECTADIV', 'Geo_UA', 'Geo_UACP', 'Geo_CDCURR', \n",
    "          'Geo_SLDU', 'Geo_SLDL', 'Geo_VTD', 'Geo_ZCTA3', 'Geo_SUBMCD', 'Geo_SDELM', 'Geo_SDSEC', \n",
    "          'Geo_SDUNI', 'Geo_UR', 'Geo_PCI', 'Geo_TAZ', 'Geo_UGA', 'Geo_BTTR', 'Geo_BTBG', \n",
    "          'Geo_PUMA5', 'Geo_PUMA1', 'year','Geo_ZCTA5', 'avg_sale_to_list']\n",
    "for i in del_col:\n",
    "    del ntile_DoM_5[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set variables to hold test and training data.\n",
    "\n",
    "ntile_5_noDOM = ntile_DoM_5.drop(columns=[\"days_on_market\"])\n",
    "\n",
    "# Shuffle data frame.\n",
    "shuffled_5 = ntile_5_noDOM.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Remove blanks and NaNs\n",
    "shuffled_5.replace('', np.nan)\n",
    "shuffled_5 = shuffled_5.dropna(axis=0, how='any')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split into data and labels.\n",
    "labels_5 = shuffled_5[\"DoM_group\"]\n",
    "del shuffled_5[\"DoM_group\"]\n",
    "zip_codes_5 = shuffled_5[\"zip_code\"]\n",
    "del shuffled_5[\"zip_code\"]\n",
    "shuffled_5 = shuffled_5.apply(lambda x: pd.to_numeric(x))\n",
    "\n",
    "# Create 80% split point.\n",
    "split = int(len(shuffled_5)*0.8//1)\n",
    "\n",
    "# Store in variables.\n",
    "train_data_5 = shuffled_5[:split]\n",
    "train_labels_5 = labels_5[:split]\n",
    "test_data_5 = shuffled_5[split+1:]\n",
    "test_labels_5 = labels_5[split+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.41209654 0.1536392  0.10224103 0.04995462 0.02884485 0.02170934\n",
      " 0.01845116 0.01808665 0.0160347  0.0144235  0.01185792 0.00956218\n",
      " 0.00877921 0.00855299 0.00782783 0.0068305  0.00566613 0.00545206\n",
      " 0.00537864 0.00485091]\n",
      " percent of variance captured::  0.910239952868229\n",
      "[154.0874501   94.08465663  76.75035303  53.64825541  40.76635882\n",
      "  35.36641605  32.60465838  32.28098998  30.39472669  28.82724097\n",
      "  26.13797732  23.47179386  22.49030802  22.19865801  21.23677073\n",
      "  19.83782582  18.06804659  17.72344916  17.6037027   16.71780176]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# we want to normalize all of our data if possible\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(train_data_5)\n",
    "X_test = sc.transform(test_data_5)\n",
    "\n",
    "#now perform PCA\n",
    "pca = PCA(n_components=20)\n",
    "#pca.fit(X_train)\n",
    "\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(\" percent of variance captured:: \", np.sum(pca.explained_variance_ratio_))\n",
    "print(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our PCA model w/Random Forest and no days on market :  0.10465116279069768\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(max_depth=32, random_state=0)\n",
    "classifier.fit(X_train, train_labels)\n",
    "\n",
    "# Predicting the Test set results\n",
    "predicted_labels = classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate number of incorrect values for each model.\n",
    "num_wrong = sum(test_labels != predicted_labels)\n",
    "accuracy = (len(test_labels) - num_wrong) / len(test_labels)\n",
    "print(\"Accuracy of our PCA model w/Random Forest and no days on market : \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe XGBoost will give us something better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 5 2 7 4 6 5 7 7 8 9 5 2 6 2 7 2 7 5 6 2 2 1 2 1 7 8 2 5 8 3 6 5 1\n",
      " 7 8 2 7 2 5 1 7 3 1 9 5 4 8 4 6 8 1 8 5 4 5 5 2 1 8 5 2 5 1 5 3 8 7 5 9 5\n",
      " 7 6 4 5 4 3 3 5 4 6 1 1]\n",
      "0.4069767441860465\n",
      "Thresh=0.000, n=151, Accuracy: 40.70%\n",
      "Thresh=0.001, n=150, Accuracy: 41.86%\n",
      "Thresh=0.001, n=149, Accuracy: 41.86%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-238-c823539451e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mselection_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mselection_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselect_X_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[1;31m# eval model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mselect_X_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mids\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[0;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 732\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"objective\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mids\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[0;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mids\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mids\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[1;32m-> 1109\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m   1110\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(train_data, train_labels)\n",
    "# make predictions for test data and evaluate\n",
    "y_pred = model.predict(test_data)\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "# Calculate number of incorrect values for each model.\n",
    "num_wrong = sum(test_labels != y_pred)\n",
    "accuracy = (len(test_labels) - num_wrong) / len(test_labels)\n",
    "print(accuracy)\n",
    "\n",
    "# accuracy = accuracy_score(test_data, predictions)\n",
    "# print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "# Fit model using each importance as a threshold\n",
    "\n",
    "thresholds = np.sort(model.feature_importances_)\n",
    "\n",
    "for thresh in thresholds:\n",
    "    if(thresh > 0.00001):\n",
    "        # select features using threshold\n",
    "        selection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "        select_X_train = selection.transform(train_data)\n",
    "        # train model\n",
    "        selection_model = XGBClassifier()\n",
    "        selection_model.fit(select_X_train, train_labels)\n",
    "        # eval model\n",
    "        select_X_test = selection.transform(test_data)\n",
    "        y_pred = selection_model.predict(select_X_test)\n",
    "        #predictions = [round(value) for value in y_pred]\n",
    "        accuracy = accuracy_score(test_labels, y_pred)\n",
    "        print(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making 10 bins and dropping them though doesn't seem to do too much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create avg_sale_to_list groups (split into 5)\n",
    "ntile_10_DoM_query = \"\"\"SELECT *, NTILE(10) OVER (\n",
    "                            ORDER BY days_on_market ASC) as DoM_10_group      \n",
    "                        FROM sales_demo\"\"\"\n",
    "ntile_10_DoM = ps.sqldf(ntile_10_DoM_query, locals())\n",
    "\n",
    "\n",
    "#see group maximums\n",
    "minmax_10_DoM_query = \"\"\"SELECT DoM_10_group, COUNT(zip_code), MAX(days_on_market)\n",
    "                        FROM ntile_10_DoM\n",
    "                        GROUP BY DoM_10_group\"\"\"\n",
    "minmax_10_DoM = ps.sqldf(minmax_10_DoM_query, locals())\n",
    "\n",
    "#Delete non-numeric and unused columns.\n",
    "del_col = ['Geo_FIPS', 'Geo_GEOID', 'Geo_NAME', 'Geo_QName', 'Geo_STUSAB', 'Geo_SUMLEV', \n",
    "           'Geo_GEOCOMP', 'Geo_FILEID', 'Geo_LOGRECNO', 'Geo_US', 'Geo_REGION', 'Geo_DIVISION', \n",
    "          'Geo_STATECE', 'Geo_STATE', 'Geo_COUNTY', 'Geo_COUSUB', 'Geo_PLACE', 'Geo_PLACESE', \n",
    "          'Geo_TRACT', 'Geo_BLKGRP', 'Geo_CONCIT', 'Geo_AIANHH', 'Geo_AIANHHFP', 'Geo_AIHHTLI', \n",
    "          'Geo_AITSCE', 'Geo_AITS', 'Geo_ANRC', 'Geo_CBSA', 'Geo_CSA', 'Geo_METDIV', 'Geo_MACC', \n",
    "          'Geo_MEMI', 'Geo_NECTA', 'Geo_CNECTA', 'Geo_NECTADIV', 'Geo_UA', 'Geo_UACP', 'Geo_CDCURR', \n",
    "          'Geo_SLDU', 'Geo_SLDL', 'Geo_VTD', 'Geo_ZCTA3', 'Geo_SUBMCD', 'Geo_SDELM', 'Geo_SDSEC', \n",
    "          'Geo_SDUNI', 'Geo_UR', 'Geo_PCI', 'Geo_TAZ', 'Geo_UGA', 'Geo_BTTR', 'Geo_BTBG', \n",
    "          'Geo_PUMA5', 'Geo_PUMA1', 'year','Geo_ZCTA5', 'avg_sale_to_list']\n",
    "for i in del_col:\n",
    "    del ntile_10_DoM[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try adding in some new columns around availability and affordability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntile_10_noDOM = ntile_10_DoM.drop(columns=[\"days_on_market\"])\n",
    "ntile_10_noDOM['Availability'] = ntile_10_noDOM.apply(lambda row: row.new_listings / np.maximum(1.0, row.SE_A00002_002), axis = 1)\n",
    "ntile_10_noDOM['cost_ratio'] = ntile_10_noDOM.apply(lambda row: row.med_sale_price / np.maximum(1.0, row.SE_A14024_001), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>med_sale_price</th>\n",
       "      <th>homes_sold</th>\n",
       "      <th>new_listings</th>\n",
       "      <th>inventory</th>\n",
       "      <th>ppsf</th>\n",
       "      <th>SE_A00001_001</th>\n",
       "      <th>SE_A00002_001</th>\n",
       "      <th>SE_A00002_002</th>\n",
       "      <th>SE_A00002_003</th>\n",
       "      <th>SE_A00003_001</th>\n",
       "      <th>...</th>\n",
       "      <th>SE_A10066_001</th>\n",
       "      <th>SE_A10066_002</th>\n",
       "      <th>SE_A10066_003</th>\n",
       "      <th>SE_A10066_004</th>\n",
       "      <th>SE_A10066_005</th>\n",
       "      <th>SE_A10066_006</th>\n",
       "      <th>SE_A10066_007</th>\n",
       "      <th>SE_A10066_008</th>\n",
       "      <th>Availability</th>\n",
       "      <th>cost_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.156667e+05</td>\n",
       "      <td>711</td>\n",
       "      <td>813.0</td>\n",
       "      <td>27.583333</td>\n",
       "      <td>215.500000</td>\n",
       "      <td>21185</td>\n",
       "      <td>21185</td>\n",
       "      <td>2747.2800</td>\n",
       "      <td>7.711264</td>\n",
       "      <td>7.829637</td>\n",
       "      <td>...</td>\n",
       "      <td>7232</td>\n",
       "      <td>1601</td>\n",
       "      <td>2368</td>\n",
       "      <td>1298</td>\n",
       "      <td>997</td>\n",
       "      <td>554</td>\n",
       "      <td>248</td>\n",
       "      <td>166</td>\n",
       "      <td>0.295929</td>\n",
       "      <td>11.983853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.925833e+05</td>\n",
       "      <td>1565</td>\n",
       "      <td>2056.0</td>\n",
       "      <td>124.666667</td>\n",
       "      <td>414.333333</td>\n",
       "      <td>45282</td>\n",
       "      <td>45282</td>\n",
       "      <td>6093.9050</td>\n",
       "      <td>7.430703</td>\n",
       "      <td>7.442647</td>\n",
       "      <td>...</td>\n",
       "      <td>18236</td>\n",
       "      <td>5355</td>\n",
       "      <td>6380</td>\n",
       "      <td>3029</td>\n",
       "      <td>2494</td>\n",
       "      <td>786</td>\n",
       "      <td>126</td>\n",
       "      <td>66</td>\n",
       "      <td>0.337386</td>\n",
       "      <td>13.831749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8.625000e+05</td>\n",
       "      <td>528</td>\n",
       "      <td>589.0</td>\n",
       "      <td>31.333333</td>\n",
       "      <td>451.750000</td>\n",
       "      <td>19035</td>\n",
       "      <td>19035</td>\n",
       "      <td>722.4431</td>\n",
       "      <td>26.348097</td>\n",
       "      <td>26.865210</td>\n",
       "      <td>...</td>\n",
       "      <td>7306</td>\n",
       "      <td>1831</td>\n",
       "      <td>2447</td>\n",
       "      <td>1269</td>\n",
       "      <td>1192</td>\n",
       "      <td>388</td>\n",
       "      <td>118</td>\n",
       "      <td>61</td>\n",
       "      <td>0.815289</td>\n",
       "      <td>16.806313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.632500e+05</td>\n",
       "      <td>2640</td>\n",
       "      <td>3409.0</td>\n",
       "      <td>191.833333</td>\n",
       "      <td>184.916667</td>\n",
       "      <td>77006</td>\n",
       "      <td>77006</td>\n",
       "      <td>392.5612</td>\n",
       "      <td>196.163034</td>\n",
       "      <td>197.044800</td>\n",
       "      <td>...</td>\n",
       "      <td>21577</td>\n",
       "      <td>4127</td>\n",
       "      <td>4489</td>\n",
       "      <td>3921</td>\n",
       "      <td>3871</td>\n",
       "      <td>2657</td>\n",
       "      <td>1352</td>\n",
       "      <td>1160</td>\n",
       "      <td>8.683996</td>\n",
       "      <td>16.297282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.045250e+06</td>\n",
       "      <td>965</td>\n",
       "      <td>1220.0</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>518.000000</td>\n",
       "      <td>25218</td>\n",
       "      <td>25218</td>\n",
       "      <td>6945.3490</td>\n",
       "      <td>3.630919</td>\n",
       "      <td>3.645880</td>\n",
       "      <td>...</td>\n",
       "      <td>11491</td>\n",
       "      <td>4060</td>\n",
       "      <td>4264</td>\n",
       "      <td>1779</td>\n",
       "      <td>1106</td>\n",
       "      <td>214</td>\n",
       "      <td>20</td>\n",
       "      <td>48</td>\n",
       "      <td>0.175657</td>\n",
       "      <td>15.544138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 169 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    med_sale_price  homes_sold  new_listings   inventory        ppsf  \\\n",
       "10    3.156667e+05         711         813.0   27.583333  215.500000   \n",
       "13    6.925833e+05        1565        2056.0  124.666667  414.333333   \n",
       "17    8.625000e+05         528         589.0   31.333333  451.750000   \n",
       "18    2.632500e+05        2640        3409.0  191.833333  184.916667   \n",
       "19    1.045250e+06         965        1220.0   72.000000  518.000000   \n",
       "\n",
       "    SE_A00001_001  SE_A00002_001  SE_A00002_002  SE_A00002_003  SE_A00003_001  \\\n",
       "10          21185          21185      2747.2800       7.711264       7.829637   \n",
       "13          45282          45282      6093.9050       7.430703       7.442647   \n",
       "17          19035          19035       722.4431      26.348097      26.865210   \n",
       "18          77006          77006       392.5612     196.163034     197.044800   \n",
       "19          25218          25218      6945.3490       3.630919       3.645880   \n",
       "\n",
       "    ...  SE_A10066_001  SE_A10066_002  SE_A10066_003  SE_A10066_004  \\\n",
       "10  ...           7232           1601           2368           1298   \n",
       "13  ...          18236           5355           6380           3029   \n",
       "17  ...           7306           1831           2447           1269   \n",
       "18  ...          21577           4127           4489           3921   \n",
       "19  ...          11491           4060           4264           1779   \n",
       "\n",
       "    SE_A10066_005  SE_A10066_006  SE_A10066_007  SE_A10066_008  Availability  \\\n",
       "10            997            554            248            166      0.295929   \n",
       "13           2494            786            126             66      0.337386   \n",
       "17           1192            388            118             61      0.815289   \n",
       "18           3871           2657           1352           1160      8.683996   \n",
       "19           1106            214             20             48      0.175657   \n",
       "\n",
       "    cost_ratio  \n",
       "10   11.983853  \n",
       "13   13.831749  \n",
       "17   16.806313  \n",
       "18   16.297282  \n",
       "19   15.544138  \n",
       "\n",
       "[5 rows x 169 columns]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set variables to hold test and training data.\n",
    "\n",
    "# Shuffle data frame.\n",
    "shuffled_10 = ntile_10_noDOM.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Remove blanks and NaNs\n",
    "shuffled_10.replace('', np.nan)\n",
    "shuffled_10 = shuffled_10.dropna(axis=0, how='any')\n",
    "\n",
    "# Split into data and labels.\n",
    "labels_10 = shuffled_10[\"DoM_10_group\"]\n",
    "del shuffled_10[\"DoM_10_group\"]\n",
    "zip_codes = shuffled_10[\"zip_code\"]\n",
    "del shuffled_10[\"zip_code\"]\n",
    "shuffled_10 = shuffled_10.apply(lambda x: pd.to_numeric(x))\n",
    "\n",
    "# Create 70% split point.\n",
    "split_10 = int(len(shuffled_10)*0.7//1)\n",
    "\n",
    "# Store in variables.\n",
    "train_data_10 = shuffled_10[:split]\n",
    "train_labels_10 = labels_10[:split]\n",
    "test_data_10 = shuffled_10[split+1:]\n",
    "test_labels_10 = labels_10[split+1:]\n",
    "\n",
    "train_data_10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 3 7 6 2 1 2 7 2 2 2 2 1 8 6 5 6 2 2 1 2 5 5 1 8 5 6 5 7 2 4 1 2 4 4 3\n",
      " 1 1 1 6 8 5 6 4 7 3 4 6 7 1 2 2 4 1 6 3 6 9 3 1 2 4 6 7 2 8 7 8 3 8 3 6 4\n",
      " 2 7 1 6 1 4 4 7 6 4 1 8]\n",
      "0.3023255813953488\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(train_data_10, train_labels_10)\n",
    "# make predictions for test data and evaluate\n",
    "y_pred = model.predict(test_data_10)\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "# Calculate number of incorrect values for each model.\n",
    "num_wrong = sum(test_labels_10 != y_pred)\n",
    "accuracy = (len(test_labels_10) - num_wrong) / len(test_labels_10)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\winbase\\Anaconda3\\envs\\mids\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 3 3 6 6 2 1 2 4 2 2 2 2 1 8 6 5 6 2 2 1 8 7 8 1 8 5 6 4 3 3 4 1 8 4 4 4\n",
      " 1 1 1 6 8 5 7 4 7 3 4 6 7 1 2 2 4 1 7 3 6 9 1 1 2 5 6 7 7 8 7 9 3 8 3 8 2\n",
      " 1 6 1 6 1 8 8 7 6 4 1 8]\n",
      "0.3372093023255814\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "def CreateBalancedSampleWeights(y_train, largest_class_weight_coef):\n",
    "    classes = np.unique(y_train, axis = 0)\n",
    "    classes.sort()\n",
    "    class_samples = np.bincount(y_train)\n",
    "    total_samples = class_samples.sum()\n",
    "    n_classes = len(class_samples)\n",
    "    weights = total_samples / (n_classes * class_samples * 1.0)\n",
    "    class_weight_dict = {key : value for (key, value) in zip(classes, weights)}\n",
    "    class_weight_dict[classes[1]] = class_weight_dict[classes[1]] * largest_class_weight_coef\n",
    "    sample_weights = [class_weight_dict[y] for y in y_train]\n",
    "    return sample_weights\n",
    "\n",
    "\n",
    "#pass labels as numpy array\n",
    "weight = CreateBalancedSampleWeights(train_labels_10, 20.0)\n",
    "\n",
    "#And then use it like this\n",
    "xg = XGBClassifier(n_estimators=3000, weights = weight, max_depth=100)\n",
    "\n",
    "xg.fit(train_data_10, train_labels_10)\n",
    "# make predictions for test data and evaluate\n",
    "y_pred = xg.predict(test_data_10)\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "# Calculate number of incorrect values for each model.\n",
    "num_wrong = sum(test_labels_10 != y_pred)\n",
    "accuracy = (len(test_labels_10) - num_wrong) / len(test_labels_10)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "thresholds = np.sort(model.feature_importances_)\n",
    "\n",
    "for thresh in thresholds:\n",
    "    if(thresh > 0.001):\n",
    "        # select features using threshold\n",
    "        selection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "        select_X_train = selection.transform(train_data_10)\n",
    "        # train model\n",
    "        selection_model = XGBClassifier()\n",
    "        selection_model.fit(select_X_train, train_labels)\n",
    "        # eval model\n",
    "        select_X_test = selection.transform(test_data_10)\n",
    "        y_pred = selection_model.predict(select_X_test)\n",
    "        #predictions = [round(value) for value in y_pred]\n",
    "        accuracy = accuracy_score(test_labels_10, y_pred)\n",
    "        print(\"Thresh=%.3f, n=%d, Accuracy: %.4f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can GridSearch help us find good MLP params?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "\n",
    "parameters = {'solver': ['lbfgs'], 'max_iter': [100, 300, 500, 700, 1200, 2000 ], 'alpha': 10.0 ** -np.arange(1, 10), 'hidden_layer_sizes':np.arange(3, 15), 'random_state':[0,1,2,3,4,5,6,7,8,9]}\n",
    "\n",
    "train_data_scaled = preprocessing.scale(train_data_10)\n",
    "\n",
    "clf = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1)\n",
    "\n",
    "clf.fit(train_data_10, train_labels_10)\n",
    "print(clf.score(test_data_10, test_labels_10))\n",
    "print(clf.best_params_)\n",
    "\n",
    "y_pred = clf.predict(test_data_10)\n",
    "\n",
    "# Calculate number of incorrect values for each model.\n",
    "num_wrong = sum(test_labels_10 != y_pred)\n",
    "accuracy = (len(test_labels_10) - num_wrong) / len(test_labels_10)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
